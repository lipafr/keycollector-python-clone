#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è WordStat Parser —Å KeywordManager
–î–æ–±–∞–≤–ª—è–µ—Ç –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤
"""

import pandas as pd
import re
from typing import List, Dict, Any, Optional
import logging

class FixedWordstatIntegration:
    """–ö–ª–∞—Å—Å –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –í–°–ï–• –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ WordStat –≤ KeywordManager"""
    
    def __init__(self, wordstat_parser):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –æ–±—ä–µ–∫—Ç–æ–º WordstatParser
        
        Args:
            wordstat_parser: –û–±—ä–µ–∫—Ç WordstatParser —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        self.parser = wordstat_parser
        self.logger = logging.getLogger(__name__)
        
    def add_all_keywords_to_manager(self, keyword_manager, 
                                  min_frequency: int = 0,
                                  exclude_patterns: List[str] = None,
                                  max_keywords: Optional[int] = None):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ KeywordManager
        
        Args:
            keyword_manager: –û–±—ä–µ–∫—Ç KeywordManager
            min_frequency: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0 - –≤—Å–µ)
            exclude_patterns: –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, ['snowrunner'])
            max_keywords: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (None = –≤—Å–µ)
        
        Returns:
            dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        """
        if not hasattr(self.parser, 'keywords') or not self.parser.keywords:
            raise ValueError("WordstatParser –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ.")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if exclude_patterns is None:
            exclude_patterns = []
        
        # –°—á–µ—Ç—á–∏–∫–∏
        added_count = 0
        skipped_count = 0
        duplicate_count = 0
        error_count = 0
        
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
        print(f"üìä –í—Å–µ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(self.parser.keywords)}")
        print(f"‚öôÔ∏è –§–∏–ª—å—Ç—Ä—ã:")
        print(f"   - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å: {min_frequency}")
        print(f"   - –ò—Å–∫–ª—é—á–∞–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {exclude_patterns if exclude_patterns else '–Ω–µ—Ç'}")
        print(f"   - –ú–∞–∫—Å–∏–º—É–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {max_keywords if max_keywords else '–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π'}")
        print()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        for i, keyword_data in enumerate(self.parser.keywords, 1):
            try:
                keyword = keyword_data.get('keyword', '').strip()
                frequency = keyword_data.get('frequency', 0)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                if not keyword:
                    skipped_count += 1
                    continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
                if frequency < min_frequency:
                    skipped_count += 1
                    continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –∏—Å–∫–ª—é—á–∞–µ–º—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
                should_exclude = False
                for pattern in exclude_patterns:
                    if pattern.lower() in keyword.lower():
                        should_exclude = True
                        break
                
                if should_exclude:
                    skipped_count += 1
                    continue
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
                if max_keywords and added_count >= max_keywords:
                    print(f"‚èπÔ∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {max_keywords}")
                    break
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                    existing_keyword = keyword_manager.get_keyword(keyword)
                    
                    if existing_keyword:
                        # –û–±–Ω–æ–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ –æ–Ω–∞ –±–æ–ª—å—à–µ
                        if frequency > existing_keyword.get('frequency', 0):
                            keyword_manager.update_keyword_frequency(keyword, frequency)
                            print(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ: '{keyword}' - {frequency} –∑–∞–ø—Ä–æ—Å–æ–≤")
                        else:
                            duplicate_count += 1
                    else:
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
                        keyword_manager.add_keyword(
                            keyword=keyword,
                            frequency=frequency,
                            cpc=0.0  # CPC –ø–æ–∫–∞ –Ω–µ –∏–∑–≤–µ—Å—Ç–µ–Ω
                        )
                        added_count += 1
                        
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 50 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                        if added_count % 50 == 0:
                            print(f"üìà –î–æ–±–∞–≤–ª–µ–Ω–æ: {added_count} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤...")
                        
                except Exception as e:
                    error_count += 1
                    self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ '{keyword}': {e}")
                    
            except Exception as e:
                error_count += 1
                self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ #{i}: {e}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            'total_processed': len(self.parser.keywords),
            'added': added_count,
            'skipped': skipped_count,
            'duplicates': duplicate_count,
            'errors': error_count,
            'success_rate': round((added_count / len(self.parser.keywords)) * 100, 1)
        }
        
        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   - –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['total_processed']}")
        print(f"   - –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {stats['added']}")
        print(f"   - –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']}")
        print(f"   - –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates']}")
        print(f"   - –û—à–∏–±–æ–∫: {stats['errors']}")
        print(f"   - –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {stats['success_rate']}%")
        
        return stats
    
    def add_keywords_by_groups(self, keyword_manager, groups_data):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ –≥—Ä—É–ø–ø–∞–º
        
        Args:
            keyword_manager: –û–±—ä–µ–∫—Ç KeywordManager
            groups_data: –î–∞–Ω–Ω—ã–µ –æ –≥—Ä—É–ø–ø–∞—Ö –∏–∑ parser.get_keyword_groups()
        """
        added_count = 0
        
        print(f"üìÅ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ –≥—Ä—É–ø–ø–∞–º...")
        print(f"üóÇÔ∏è –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø: {len(groups_data)}")
        
        for group_name, keywords_in_group in groups_data.items():
            print(f"\nüìÇ –ì—Ä—É–ø–ø–∞: '{group_name}' ({len(keywords_in_group)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)")
            
            for keyword_data in keywords_in_group:
                try:
                    keyword = keyword_data['keyword']
                    frequency = keyword_data['frequency']
                    
                    keyword_manager.add_keyword(
                        keyword=keyword,
                        frequency=frequency,
                        cpc=0.0,
                        group=group_name  # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–µ
                    )
                    
                    added_count += 1
                    print(f"   ‚úÖ '{keyword}' ({frequency} –∑–∞–ø—Ä–æ—Å–æ–≤)")
                    
                except Exception as e:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ '{keyword}': {e}")
        
        print(f"\nüéâ –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ –≥—Ä—É–ø–ø–∞–º: {added_count} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        return added_count


def fix_wordstat_integration_test():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à–∏ –ø—É—Ç–∏)
        from wordstat_parser import WordstatParser
        from keyword_manager import KeywordManager
        
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ WordStat...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        parser = WordstatParser()
        parser.parse_file("data/input/–≥–µ–æ–ª–æ–≥–æ—Ä–∞–∑–≤–µ–¥–∫–∞ wordstat.xlsx")
        
        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_manager = KeywordManager()
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        fixed_integration = FixedWordstatIntegration(parser)
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: –î–æ–±–∞–≤–∏—Ç—å –í–°–ï –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤
        print("\n" + "="*60)
        print("–í–ê–†–ò–ê–ù–¢ 1: –î–æ–±–∞–≤–ª—è–µ–º –í–°–ï 352 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞")
        print("="*60)
        
        stats1 = fixed_integration.add_all_keywords_to_manager(
            keyword_manager=keyword_manager,
            min_frequency=0,  # –ë–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
            exclude_patterns=[],  # –ë–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
            max_keywords=None  # –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        total_in_manager = len(keyword_manager.get_all_keywords())
        print(f"\nüéØ –ò–¢–û–ì–û –≤ –º–µ–Ω–µ–¥–∂–µ—Ä–µ: {total_in_manager} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: –¢–æ–ª—å–∫–æ –±–µ–∑ –∏–≥—Ä–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        keyword_manager_filtered = KeywordManager()
        
        print("\n" + "="*60)
        print("–í–ê–†–ò–ê–ù–¢ 2: –ë–µ–∑ –∏–≥—Ä–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (snowrunner)")
        print("="*60)
        
        stats2 = fixed_integration.add_all_keywords_to_manager(
            keyword_manager=keyword_manager_filtered,
            min_frequency=0,
            exclude_patterns=['snowrunner'],  # –ò—Å–∫–ª—é—á–∞–µ–º –∏–≥—Ä–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            max_keywords=None
        )
        
        total_filtered = len(keyword_manager_filtered.get_all_keywords())
        print(f"\nüéØ –ò–¢–û–ì–û –±–µ–∑ –∏–≥—Ä–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {total_filtered} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
        
        return True, stats1, stats2
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–µ: {e}")
        return False, None, None


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞
    success, stats1, stats2 = fix_wordstat_integration_test()
    
    if success:
        print(f"\nüéâ –¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {stats1['added']}")
        print(f"üìä –î–æ–±–∞–≤–ª–µ–Ω–æ –±–µ–∑ –∏–≥—Ä–æ–≤—ã—Ö: {stats2['added']}")
    else:
        print(f"‚ùå –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π")